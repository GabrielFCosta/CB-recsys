{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO83+9FLYDCFGgQEz+Cet1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielFCosta/TCC-BSI-2023.1/blob/main/old/matc97_gabriel_teste_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g1zLg3Ipx77",
        "outputId": "0cca08c2-e6d2-4192-de58-6352c1c4f5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import math\n",
        "from urllib.request import urlopen\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWcc_InWq1HI",
        "outputId": "6e5c8ce0-de74-4ea8-8df2-ebcf1fca1203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções do reconhecimento de entidades nomeadas:\n",
        "\n",
        "# Remove pontuação, utilizada pela função seguinte de pré-processamento\n",
        "def remover_pontuacao(tokens):\n",
        "  for token in tokens:\n",
        "    if token in string.punctuation:\n",
        "      tokens.remove(token)\n",
        "# Função de pré-processamento\n",
        "def preprocess(text):\n",
        "    text = word_tokenize(text)\n",
        "    remover_pontuacao(text)\n",
        "    return pos_tag(text)\n",
        "\n",
        "# Listas para coletar labels e entidades pra cada sentença lida do texto\n",
        "codes = []\n",
        "words = []\n",
        "def  clearlists():\n",
        "  if len(words) > 0:\n",
        "    words.clear()\n",
        "  if len(codes) > 0:\n",
        "    codes.clear()\n",
        "\n",
        "# Função de reconhecimento de entidades nomeadas\n",
        "def ner(sent):\n",
        "  ne_tree = nltk.ne_chunk(sent)\n",
        "  for chunk in ne_tree:\n",
        "    if hasattr(chunk,\"label\"):\n",
        "      word =' '.join(c[0] for c in chunk)\n",
        "     #print(chunk.label(),word)\n",
        "      codes.append(chunk.label())\n",
        "      words.append(word)\n",
        "\n",
        "# Funções para criar um dataframe contendo entidades, labels e suas frequencias \n",
        "def returndataset():\n",
        "  dataset = datajoin(words,codes)\n",
        "  # calcula frequências das entidades e adiciona nova coluna\n",
        "  dataset['freq'] = dataset.groupby(['entity','label'])['entity'].transform('count')\n",
        "  return dataclean(dataset)\n",
        "def datajoin(words,codes):\n",
        "  # junta as duas listas de entidades e labels e um dataframe\n",
        "  dataset = pd.DataFrame(zip(words,codes),columns =['entity', 'label'])\n",
        "  clearlists()\n",
        "  return dataset\n",
        "def dataclean(dataset):\n",
        "  # remove linhas duplicatas e atualiza index\n",
        "  dataset.drop_duplicates(inplace= True)\n",
        "  dataset.reset_index()\n",
        "  return dataset\n",
        "\n",
        "# Calcula TF (Term frequency) das entidades e adiciona coluna ao dataframe\n",
        "def calculaTF(dataset):\n",
        "  tfs = []\n",
        "  # frequência do termo mais frequente no documento \n",
        "  max = dataset['freq'].max()\n",
        "  # calcula TF para cada entidade no documento\n",
        "  for value in dataset['freq']:\n",
        "    tfs.append(value/max)\n",
        "  dataset['TF'] = tfs\n",
        "  return dataset\n",
        "\n",
        "# Calcula IDF (Inverse Document Frequency) e retorna dataframe\n",
        "def retornaIDF(datasets,c):\n",
        "  clearlists()\n",
        "  # coleta entidades do primeiro documento\n",
        "  for index,row in datasets[0].iterrows():\n",
        "     words.append(row['entity'])\n",
        "     codes.append(row['label'])\n",
        "  # coleta entidades do documento a ser comparado\n",
        "  for index, row in datasets[c].iterrows():\n",
        "      words.append(row['entity'])\n",
        "      codes.append(row['label'])\n",
        "  # Produz dataframe de todas as entidades e respectivas frequências no corpus\n",
        "  idfs = returndataset()\n",
        "  # total de documentos no corpus será sempre 2 (dois documentos sendo comparados)\n",
        "  N = 2\n",
        "  aux = []\n",
        "  # Calcula IDF (utilizando ln) para cada entidade e adiciona coluna ao dataframe\n",
        "  for index, row in idfs.iterrows():\n",
        "    aux.append(math.log(N/row['freq']))\n",
        "  idfs['IDF'] = aux\n",
        "  return idfs \n",
        "\n",
        "# Retorna dataframe com pesos para comparação entre documentos\n",
        "def calculaPeso(datasets,indexdoc,indexidf):\n",
        " idf = retornaIDF(datasets,indexidf)\n",
        " # produz um dataframe composto pela união das TFs e IDFs de todas entidades dos dois documentos.  \n",
        " #wij = pd.merge(datasets[indexdoc].loc[:,datasets[indexdoc].columns!=\"freq\"], idf.loc[:,idf.columns!=\"freq\"], on=['entity','label'], how='outer')\n",
        " wij = pd.merge(datasets[indexdoc], idf.loc[:,idf.columns!=\"freq\"], on=['entity','label'], how='outer')\n",
        " # preenche valores de IDF NaN com 0\n",
        " wij = dataclean(wij).fillna (0)\n",
        " # cria outra coluna com o valores dos pesos: coluna IDF * coluna TF\n",
        " wij[\"Wij\"] = wij['IDF'] * wij['TF']\n",
        " # retorna dataframe ordenado\n",
        " return wij.sort_values(by=['entity','label'])\n",
        "\n",
        "# similaridade de jaccard simplificada \n",
        "def similaridade_jaccard(freqA, freqN):\n",
        "    intersecao = 0\n",
        "    c = 0\n",
        "    for x in freqA:\n",
        "      if x == freqN[c]:\n",
        "        intersecao += 1\n",
        "      c += 1\n",
        "    print(\"interseção:\",intersecao)\n",
        "    uniao = len(freqA)\n",
        "    print(\"união:\",uniao)\n",
        "    print(\"jaccard simplificada:\",intersecao/uniao)\n",
        "\n",
        "# fórmula da similaridade de jaccard utilizando frequências absolutas\n",
        "def jaccard(freqA, freqN):\n",
        "  numerador = returnumerador(freqA, freqN)\n",
        "  denominador = 0 \n",
        "  for valor in freqA:\n",
        "    denominador += valor * valor\n",
        "  for valor in freqN:\n",
        "    denominador += valor * valor\n",
        "  denominador = denominador - numerador\n",
        "  print(\"jaccard fórmula:\", numerador/denominador)\n",
        "\n",
        "def returnumerador(freqA, freqN):\n",
        "  numerador = 0\n",
        "  c = 0\n",
        "  for valor in freqA:\n",
        "    numerador += valor * freqN[c]\n",
        "    c+=1\n",
        "  return numerador\n",
        "  \n",
        "# (false)\n",
        "def cosine(freqA, freqN):\n",
        "  numerador = returnumerador(freqA, freqN)\n",
        "  den0 = 0\n",
        "  den1 = 1\n",
        "  for val0 in freqA:\n",
        "    den0 += val0 * val0\n",
        "  for val1 in freqN:\n",
        "    den1 += val1 * val1\n",
        "  denominador = math.sqrt(den0) * math.sqrt(den1)\n",
        "  print(\"cosine:\", numerador/denominador)\n",
        "\n"
      ],
      "metadata": {
        "id": "e1C1u6qgrWtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execução do reconhecimento de entidades nomeadadas:\n",
        "\n",
        "#urls = [\"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia.txt\",\n",
        "#        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia1.txt\",\n",
        "#        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia2.txt\"]\n",
        "# Lista que vai conter dataframes de entidades (corpus)\n",
        "datasets = []\n",
        "\n",
        "urls = [\"Airbnb will shut down its listings in China after two years of lockdowns in the country\",\n",
        "        \"Starting this summer, Airbnb will take down its listings and offers for hosted experiences in China.\",\n",
        "        \"International brands, from Apple to Estee Lauder, have warned of the financial impact of the restrictions.\"]\n",
        "# Pra cada url\n",
        "for url in urls:\n",
        "  #data = urlopen(url)\n",
        "  #ex = data.read().decode('utf-8')\n",
        "  ex=nltk.sent_tokenize(url)\n",
        "  print(ex)\n",
        "  # Detectando entidades de nomeadas por sentença\n",
        "  for sentence in ex:\n",
        "    # Sentença Pré-processada\n",
        "    text = preprocess(sentence)\n",
        "    print(text)\n",
        "    # Entidades Nomeadas na Sentença\n",
        "    ner(text)\n",
        "  # Adiciona dataframe de entidades nomeadas do texto + labels, frequências e TF\n",
        "  datasets.append(calculaTF(returndataset()))\n",
        "\n",
        "c = 0\n",
        "for dataset in datasets:\n",
        "  print(\"\\nDocumento:\",c)\n",
        "  print(dataset)\n",
        "  c+=1\n",
        "for x in range(c-1):\n",
        "  print(\"\\nTabela IDF para documentos 0 e\",x+1)\n",
        "  print(retornaIDF(datasets,x+1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b559ae-2a59-418e-92ac-5d5b5e56ae56",
        "id": "3qeGsrP5hV1_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Airbnb will shut down its listings in China after two years of lockdowns in the country']\n",
            "[('Airbnb', 'NNP'), ('will', 'MD'), ('shut', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('in', 'IN'), ('China', 'NNP'), ('after', 'IN'), ('two', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('lockdowns', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('country', 'NN')]\n",
            "['Starting this summer, Airbnb will take down its listings and offers for hosted experiences in China.']\n",
            "[('Starting', 'VBG'), ('this', 'DT'), ('summer', 'NN'), ('Airbnb', 'NNP'), ('will', 'MD'), ('take', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('and', 'CC'), ('offers', 'NNS'), ('for', 'IN'), ('hosted', 'JJ'), ('experiences', 'NNS'), ('in', 'IN'), ('China', 'NNP')]\n",
            "['International brands, from Apple to Estee Lauder, have warned of the financial impact of the restrictions.']\n",
            "[('International', 'NNP'), ('brands', 'NNS'), ('from', 'IN'), ('Apple', 'NNP'), ('to', 'TO'), ('Estee', 'NNP'), ('Lauder', 'NNP'), ('have', 'VBP'), ('warned', 'VBN'), ('of', 'IN'), ('the', 'DT'), ('financial', 'JJ'), ('impact', 'NN'), ('of', 'IN'), ('the', 'DT'), ('restrictions', 'NNS')]\n",
            "\n",
            "Documento: 0\n",
            "   entity label  freq   TF\n",
            "0  Airbnb   GPE     1  1.0\n",
            "1   China   GPE     1  1.0\n",
            "\n",
            "Documento: 1\n",
            "   entity   label  freq   TF\n",
            "0  Airbnb  PERSON     1  1.0\n",
            "1   China     GPE     1  1.0\n",
            "\n",
            "Documento: 2\n",
            "         entity   label  freq   TF\n",
            "0         Apple     GPE     1  1.0\n",
            "1  Estee Lauder  PERSON     1  1.0\n",
            "\n",
            "Tabela IDF para documentos 0 e 1\n",
            "   entity   label  freq       IDF\n",
            "0  Airbnb     GPE     1  0.693147\n",
            "1   China     GPE     2  0.000000\n",
            "2  Airbnb  PERSON     1  0.693147\n",
            "\n",
            "Tabela IDF para documentos 0 e 2\n",
            "         entity   label  freq       IDF\n",
            "0        Airbnb     GPE     1  0.693147\n",
            "1         China     GPE     1  0.693147\n",
            "2         Apple     GPE     1  0.693147\n",
            "3  Estee Lauder  PERSON     1  0.693147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComparando documento 0 consigo mesmo (caso base):\")\n",
        "print(\"\\nTabela de pesos para documentos 0\")\n",
        "wij0 = calculaPeso(datasets,0,0)\n",
        "print(wij0)\n",
        "similaridade_jaccard(wij0['freq'].tolist(), wij0['freq'].tolist())\n",
        "jaccard(wij0['freq'].tolist(), wij0['freq'].tolist())\n",
        "#cosine(wij0['freq'].tolist(), wij0['freq'].tolist())\n",
        "\n",
        "print(\"\\nComparando documentos 0 e 1:\")\n",
        "print(\"\\nTabela de pesos para documentos 0\")\n",
        "wij0 = calculaPeso(datasets,0,1)\n",
        "print(wij0)\n",
        "print(\"\\nTabela de pesos para documentos 1\")\n",
        "wij1 = calculaPeso(datasets,1,1)\n",
        "print(wij1)\n",
        "print(\"\\nSimilaridade entre documentos 0 e 1:\")\n",
        "similaridade_jaccard(wij0['freq'].tolist(), wij1['freq'].tolist())\n",
        "jaccard(wij0['freq'].tolist(), wij1['freq'].tolist())\n",
        "#cosine(wij0['freq'].tolist(), wij1['freq'].tolist())\n",
        "\n",
        "print(\"\\nComparando documentos 0 e 2:\")\n",
        "print(\"\\nTabela de pesos para documentos 0\")\n",
        "wij0 = calculaPeso(datasets,0,2)\n",
        "print(wij0)\n",
        "print(\"\\nTabela de pesos para documentos 1\")\n",
        "wij1 = calculaPeso(datasets,2,2)\n",
        "print(wij1)\n",
        "print(\"\\nSimilaridade de jaccard entre documentos 0 e 2:\")\n",
        "similaridade_jaccard(wij0['freq'].tolist(), wij1['freq'].tolist())\n",
        "jaccard(wij0['freq'].tolist(), wij1['freq'].tolist())\n",
        "#cosine(wij0['freq'].tolist(), wij1['freq'].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zw49iuWWb-m",
        "outputId": "b523c33d-919b-4e0b-bef7-9969db23756c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparando documento 0 consigo mesmo (caso base):\n",
            "\n",
            "Tabela de pesos para documentos 0\n",
            "   entity label  freq   TF  IDF  Wij\n",
            "0  Airbnb   GPE     1  1.0  0.0  0.0\n",
            "1   China   GPE     1  1.0  0.0  0.0\n",
            "interseção: 2\n",
            "união: 2\n",
            "jaccard simplificada: 1.0\n",
            "jaccard fórmula: 1.0\n",
            "\n",
            "Comparando documentos 0 e 1:\n",
            "\n",
            "Tabela de pesos para documentos 0\n",
            "   entity   label  freq   TF       IDF       Wij\n",
            "0  Airbnb     GPE   1.0  1.0  0.693147  0.693147\n",
            "2  Airbnb  PERSON   0.0  0.0  0.693147  0.000000\n",
            "1   China     GPE   1.0  1.0  0.000000  0.000000\n",
            "\n",
            "Tabela de pesos para documentos 1\n",
            "   entity   label  freq   TF       IDF       Wij\n",
            "2  Airbnb     GPE   0.0  0.0  0.693147  0.000000\n",
            "0  Airbnb  PERSON   1.0  1.0  0.693147  0.693147\n",
            "1   China     GPE   1.0  1.0  0.000000  0.000000\n",
            "\n",
            "Similaridade entre documentos 0 e 1:\n",
            "interseção: 1\n",
            "união: 3\n",
            "jaccard simplificada: 0.3333333333333333\n",
            "jaccard fórmula: 0.3333333333333333\n",
            "\n",
            "Comparando documentos 0 e 2:\n",
            "\n",
            "Tabela de pesos para documentos 0\n",
            "         entity   label  freq   TF       IDF       Wij\n",
            "0        Airbnb     GPE   1.0  1.0  0.693147  0.693147\n",
            "2         Apple     GPE   0.0  0.0  0.693147  0.000000\n",
            "1         China     GPE   1.0  1.0  0.693147  0.693147\n",
            "3  Estee Lauder  PERSON   0.0  0.0  0.693147  0.000000\n",
            "\n",
            "Tabela de pesos para documentos 1\n",
            "         entity   label  freq   TF       IDF       Wij\n",
            "2        Airbnb     GPE   0.0  0.0  0.693147  0.000000\n",
            "0         Apple     GPE   1.0  1.0  0.693147  0.693147\n",
            "3         China     GPE   0.0  0.0  0.693147  0.000000\n",
            "1  Estee Lauder  PERSON   1.0  1.0  0.693147  0.693147\n",
            "\n",
            "Similaridade de jaccard entre documentos 0 e 2:\n",
            "interseção: 0\n",
            "união: 4\n",
            "jaccard simplificada: 0.0\n",
            "jaccard fórmula: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Funções da recomendação por conteúdo:\n",
        "\n",
        "# Carregando o conjunto de dados\n",
        "data_set = pd.read_csv(\"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/sample-data.csv\")\n",
        "print(data_set)\n",
        "# Utilizando o TfId para vetorizar o name dos items\n",
        "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
        "\n",
        "# Atributo utilizado: país de origem\n",
        "tfidf_matrix = tf.fit_transform(data_set['countryOfOrigin'])\n",
        "\n",
        "# Calculando a matriz de similaridade cosseno\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(cosine_similarities)\n",
        "\n",
        "# Apresenta itens com mesmo país de origem\n",
        "resultados= []\n",
        "i=0\n",
        "for sims in cosine_similarities[0]:\n",
        "    if(sims>0):\n",
        "        resultados.append(data_set.loc[i,'name'])\n",
        "    i+=1\n",
        "print(\"\\n Recomendando produtos similares a \" + resultados[0] + \",com base no país de origem\")\n",
        "for x in range(1,len(resultados)):\n",
        "    print(resultados[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwKOwPvK6dHH",
        "outputId": "095cf601-a481-4aa0-fa27-9f0f7f5ac311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                 genericName                                        name  \\\n",
            "0   1                      Butter                  Land O’Lakes Salted Butter   \n",
            "1   2              Cheddar cheese                 Kraft Cheddar Cheese Slices   \n",
            "2   3              Vanilla Yogurt  Yoplait Light ‘n Fit Nonfat Vanilla Yogurt   \n",
            "3   4  Chocolate Yogurt Ice Cream         Breyer’s Chocolate Yogurt Ice Cream   \n",
            "4   5                      Butter                        Banana Salted Butter   \n",
            "5   6                        milk                  Land O’Lakes full fat milk   \n",
            "\n",
            "  countryOfOrigin  \n",
            "0           China  \n",
            "1              UK  \n",
            "2          France  \n",
            "3           china  \n",
            "4             USA  \n",
            "5         chinese  \n",
            "[[1. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " Recomendando produtos similares a Land O’Lakes Salted Butter,com base no país de origem\n",
            "Breyer’s Chocolate Yogurt Ice Cream\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(tag):\n",
        "  # todos substantivos, adjetivos, verbos e numerais.\n",
        "  tags = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ','CD']\n",
        "  for i in tags:\n",
        "    if i == tag:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def filtertuples(text):\n",
        "  words = []\n",
        "  tags = []\n",
        "  for tuple in text:\n",
        "    if test(tuple[1]):\n",
        "      words.append(tuple[0])\n",
        "      tags.append(tuple[1])\n",
        "  palavras = pd.DataFrame(zip(words,tags),columns =['word', 'pos_tag'])\n",
        "  palavras['freq'] = palavras.groupby(['word','pos_tag'])['word'].transform('count')\n",
        "  #palavras = calculaTF(palavras)\n",
        "  palavras = dataclean(palavras)\n",
        "  return palavras\n",
        "\n",
        "def datamerge(words0,words1):\n",
        "  merged = pd.merge(words0,words1, on=['word','pos_tag'], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  doc_freq = []\n",
        "  c = 0\n",
        "  for i in merged['freq_x'].tolist():\n",
        "    if i > 0 and merged['freq_y'].tolist()[c] > 0:\n",
        "      doc_freq.append(2)\n",
        "    else:\n",
        "      doc_freq.append(1)\n",
        "    c += 1\n",
        "  merged['doc_freq'] = doc_freq\n",
        "  return merged.sort_values(by=['word','pos_tag'])\n",
        "\n",
        "def jaccard_simples(doc_freq):\n",
        "    intersecao = 0\n",
        "    c = 0\n",
        "    for x in doc_freq:\n",
        "      if x == 2:\n",
        "        intersecao += 1\n",
        "      c += 1\n",
        "    print(\"interseção:\",intersecao)\n",
        "    uniao = len(doc_freq)\n",
        "    print(\"união:\",uniao)\n",
        "    print(\"jaccard simplificada:\",intersecao/uniao)\n",
        "\n",
        "def similaridade_WP_means(str0,str1):\n",
        "  means = []\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se synset0 contém alguma definição\n",
        "    if len(syn0) > 0:\n",
        "      val = 0\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se synset1 contém alguma definição\n",
        "        if len(syn1) > 0:\n",
        "          val+= syn0[0].wup_similarity(syn1[0])\n",
        "          #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"val:\",val)\n",
        "      means.append(val/len(str1))\n",
        "  print(means)\n",
        "  print(sum(means)/len(means))\n",
        "\n",
        "def similaridade_WP_max(str0,str1):\n",
        "  means = []\n",
        "  vals = []\n",
        "  aux = 0\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se 1a palavra contém alguma definição antes de prosseguir\n",
        "    if len(syn0) != 0:\n",
        "      vals.clear()\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se 2a palavra contém alguma definição antes de calcular similaridade\n",
        "        if len(syn1) != 0:\n",
        "          aux = syn0[0].wup_similarity(syn1[0])\n",
        "          # se similaridade = 1, palavras iguais, break\n",
        "          if aux == 1:\n",
        "            break\n",
        "          # senão, adiciona valor à lista vals\n",
        "          else:\n",
        "            vals.append(aux)\n",
        "            #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"wup:\",syn0[0].wup_similarity(syn1[0]))\n",
        "      # se aux = 1, similaridade máxima = 1\n",
        "      if aux == 1:\n",
        "         means.append(aux)\n",
        "      # senão seleciona máxima da lista\n",
        "      else:\n",
        "        means.append(max(vals))\n",
        "  print(means)\n",
        "  print(sum(means)/len(means))\n",
        "\n",
        "str0 = filtertuples(preprocess(urls[0]))\n",
        "str1 = filtertuples(preprocess(urls[1]))\n",
        "str2 = filtertuples(preprocess(urls[2]))\n",
        "\n",
        "print(\"string 0:\",str0['word'].tolist())\n",
        "print(\"string 1:\",str1['word'].tolist())\n",
        "similaridade_WP_means(str0['word'].tolist(),str1['word'].tolist())\n",
        "similaridade_WP_max(str0['word'].tolist(),str1['word'].tolist())\n",
        "dt = datamerge(str0,str1)\n",
        "jaccard_simples(dt['doc_freq'].tolist())\n",
        "\n",
        "print('\\n')\n",
        "print(\"string 0:\",str0['word'].tolist())\n",
        "print(\"string 2:\",str2['word'].tolist())\n",
        "similaridade_WP_means(str0['word'].tolist(),str2['word'].tolist())\n",
        "similaridade_WP_max(str0['word'].tolist(),str2['word'].tolist())\n",
        "dt = datamerge(str0,str2)\n",
        "jaccard_simples(dt['doc_freq'].tolist())\n",
        "\n",
        "# cosine do scikit\n",
        "print('\\ncosine do scikit')\n",
        "corpus = [urls[0],urls[1],urls[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "cosine_similarity(trsfm[0:1], trsfm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVY6_bkXrBr_",
        "outputId": "e8240397-ad16-4faf-dabb-55a4bf83288c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "string 0: ['Airbnb', 'shut', 'listings', 'China', 'two', 'years', 'lockdowns', 'country']\n",
            "string 1: ['Starting', 'summer', 'Airbnb', 'take', 'listings', 'offers', 'hosted', 'experiences', 'China']\n",
            "[0.17105672105672104, 0.29106753812636166, 0.1999273783587509, 0.21296296296296294, 0.25932150638032986, 0.26241529750301684, 0.19814814814814813]\n",
            "0.22784279321947018\n",
            "[0.25, 1.0, 1.0, 0.4, 0.7142857142857143, 0.631578947368421, 0.26666666666666666]\n",
            "0.6089330469029717\n",
            "interseção: 3\n",
            "união: 14\n",
            "jaccard simplificada: 0.21428571428571427\n",
            "\n",
            "\n",
            "string 0: ['Airbnb', 'shut', 'listings', 'China', 'two', 'years', 'lockdowns', 'country']\n",
            "string 2: ['International', 'brands', 'Apple', 'Estee', 'Lauder', 'have', 'warned', 'financial', 'impact', 'restrictions']\n",
            "[0.20388888888888887, 0.18476190476190474, 0.16702548225768038, 0.172291923762512, 0.18476190476190474, 0.17561612364243942, 0.18405662964486494]\n",
            "0.1817718368171707\n",
            "[0.5, 0.2857142857142857, 0.35294117647058826, 0.26666666666666666, 0.2857142857142857, 0.4444444444444444, 0.35294117647058826]\n",
            "0.35548886221155124\n",
            "interseção: 0\n",
            "união: 18\n",
            "jaccard simplificada: 0.0\n",
            "\n",
            "cosine do scikit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.35608377, 0.15772405]])"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    }
  ]
}