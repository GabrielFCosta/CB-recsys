{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7gapNkCn2M0wrdcaVz0zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielFCosta/TCC-BSI-2023.1/blob/main/old/matc97_gabriel_teste_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g1zLg3Ipx77",
        "outputId": "5acf040e-3e8d-4bec-aae9-41a768b0413a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import math\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWcc_InWq1HI",
        "outputId": "e1c50602-b2c5-42ab-9cf2-58d0f1460de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove pontuação, utilizada pela função seguinte de pré-processamento\n",
        "def remover_pontuacao(tokens):\n",
        "  for token in tokens:\n",
        "    if token in string.punctuation:\n",
        "      tokens.remove(token)\n",
        "# Função de pré-processamento\n",
        "def preprocess(text):\n",
        "    text = word_tokenize(text)\n",
        "    remover_pontuacao(text)\n",
        "    return pos_tag(text)\n",
        "\n",
        "# Listas para coletar rótulos e entidades (palavras) do texto\n",
        "codes = []\n",
        "words = []\n",
        "def  clearlists():\n",
        "  if len(words) > 0:\n",
        "    words.clear()\n",
        "  if len(codes) > 0:\n",
        "    codes.clear()\n",
        "\n",
        "# Função de reconhecimento de entidades nomeadas (NER)\n",
        "def ner(sent):\n",
        "  ne_tree = nltk.ne_chunk(sent)\n",
        "  for chunk in ne_tree:\n",
        "    if hasattr(chunk,\"label\"):\n",
        "      word =' '.join(c[0] for c in chunk)\n",
        "     #print(chunk.label(),word)\n",
        "      codes.append(chunk.label())\n",
        "      words.append(word)\n",
        "\n",
        "# Função que retorna dataframe de entidades (palavras), rótulos e frequências \n",
        "def returndataset(col1,col2):\n",
        "  # junta listas de palavras e rótulos num dataframe\n",
        "  dataset = pd.DataFrame(zip(words,codes),columns =[col1,col2])\n",
        "  clearlists()\n",
        "  # calcula frequências das palavras e adiciona nova coluna\n",
        "  dataset['freq'] = dataset.groupby([col1,col2])[col1].transform('count')\n",
        "  return dataclean(dataset)\n",
        " \n",
        "# Remove linhas duplicatas e atualiza index\n",
        "def dataclean(dataset):\n",
        "  dataset.drop_duplicates(inplace= True)\n",
        "  dataset.reset_index()\n",
        "  return dataset\n",
        "\n",
        "# Verifica rótulo gramatical\n",
        "def test(tag):\n",
        "  # verificando todos substantivos, adjetivos, verbos e numerais cardinais.\n",
        "  tags = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ','CD']\n",
        "  #tags = ['NN','NNS','NNP','NNPS']\n",
        "  for i in tags:\n",
        "    if i == tag:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "# Filtra palavras da lista gerada pelo pos tagging e retorna dataframe\n",
        "def filtertuples(textlist,rtdata):\n",
        "  for tuple in textlist:\n",
        "    if test(tuple[1]):\n",
        "      words.append(tuple[0])\n",
        "      codes.append(tuple[1])\n",
        "  if rtdata:\n",
        "    return returndataset('word','pos_tag')\n",
        "  else:\n",
        "    aux = words.copy()\n",
        "    clearlists()\n",
        "  return aux\n",
        "\n",
        "# Retorna união de dois dataframes com \"frequência de documentos\" \n",
        "def datamerge(words0,words1,col1,col2):\n",
        "  merged = pd.merge(words0,words1, on=[col1,col2], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  doc_freq = []\n",
        "  c = 0\n",
        "  # verifica duas colunas de frequência\n",
        "  for i in merged['freq_x'].tolist():\n",
        "    # se frequência da palavras maior que 0 em ambos documentos \n",
        "    if i > 0 and merged['freq_y'].tolist()[c] > 0:\n",
        "      doc_freq.append(2)\n",
        "    else:\n",
        "      doc_freq.append(1)\n",
        "    c += 1\n",
        "  merged['doc_freq'] = doc_freq\n",
        "  return merged.sort_values(by=[col1,col2])\n",
        "\n",
        "def jaccard_simples(doc_freq):\n",
        "    intersecao = 0\n",
        "    c = 0\n",
        "    for x in doc_freq:\n",
        "      if x == 2:\n",
        "        intersecao += 1\n",
        "      c += 1\n",
        "    print(\"interseção:\",intersecao)\n",
        "    uniao = len(doc_freq)\n",
        "    print(\"união:\",uniao)\n",
        "    print(\"jaccard simplificada:\",intersecao/uniao)"
      ],
      "metadata": {
        "id": "e1C1u6qgrWtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execução do reconhecimento de entidades nomeadadas:\n",
        "\n",
        "#urls = [\"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia.txt\",\n",
        "#        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia1.txt\",\n",
        "#        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia2.txt\"]\n",
        "# Lista que vai conter dataframes de entidades (corpus)\n",
        "datasets = []\n",
        "\n",
        "urls = [\"Airbnb will shut down its listings in China after two years of lockdowns in the country Airbnb will shut down its listings in China\",\n",
        "        \"Starting this summer, Airbnb will take down its listings and offers for hosted experiences in China.\",\n",
        "        \"International brands, from Apple to Estee Lauder, have warned of the financial impact of the restrictions.\"]\n",
        "# Pra cada url\n",
        "for url in urls:\n",
        "  #data = urlopen(url)\n",
        "  #ex = data.read().decode('utf-8')\n",
        "  ex=nltk.sent_tokenize(url)\n",
        "  print(ex)\n",
        "  # Detectando entidades de nomeadas por sentença\n",
        "  for sentence in ex:\n",
        "    # Sentença Pré-processada\n",
        "    text = preprocess(sentence)\n",
        "    print(text)\n",
        "    # Entidades Nomeadas na Sentença\n",
        "    ner(text)\n",
        "  # Adiciona dataframe de entidades nomeadas do texto + labels, frequências e TF\n",
        "  datasets.append(returndataset('entity','label'))\n",
        "c = 0\n",
        "for dataset in datasets:\n",
        "  print(\"\\nstring:\",c)\n",
        "  print(dataset)\n",
        "  c += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42787c56-5755-484a-9047-7f256f2da20b",
        "id": "3qeGsrP5hV1_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Airbnb will shut down its listings in China after two years of lockdowns in the country Airbnb will shut down its listings in China']\n",
            "[('Airbnb', 'NNP'), ('will', 'MD'), ('shut', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('in', 'IN'), ('China', 'NNP'), ('after', 'IN'), ('two', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('lockdowns', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('country', 'NN'), ('Airbnb', 'NNP'), ('will', 'MD'), ('shut', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('in', 'IN'), ('China', 'NNP')]\n",
            "['Starting this summer, Airbnb will take down its listings and offers for hosted experiences in China.']\n",
            "[('Starting', 'VBG'), ('this', 'DT'), ('summer', 'NN'), ('Airbnb', 'NNP'), ('will', 'MD'), ('take', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('and', 'CC'), ('offers', 'NNS'), ('for', 'IN'), ('hosted', 'JJ'), ('experiences', 'NNS'), ('in', 'IN'), ('China', 'NNP')]\n",
            "['International brands, from Apple to Estee Lauder, have warned of the financial impact of the restrictions.']\n",
            "[('International', 'NNP'), ('brands', 'NNS'), ('from', 'IN'), ('Apple', 'NNP'), ('to', 'TO'), ('Estee', 'NNP'), ('Lauder', 'NNP'), ('have', 'VBP'), ('warned', 'VBN'), ('of', 'IN'), ('the', 'DT'), ('financial', 'JJ'), ('impact', 'NN'), ('of', 'IN'), ('the', 'DT'), ('restrictions', 'NNS')]\n",
            "\n",
            "string: 0\n",
            "   entity   label  freq\n",
            "0  Airbnb     GPE     1\n",
            "1   China     GPE     2\n",
            "2  Airbnb  PERSON     1\n",
            "\n",
            "string: 1\n",
            "   entity   label  freq\n",
            "0  Airbnb  PERSON     1\n",
            "1   China     GPE     1\n",
            "\n",
            "string: 2\n",
            "         entity   label  freq\n",
            "0         Apple     GPE     1\n",
            "1  Estee Lauder  PERSON     1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComparando documento 0 consigo mesmo (caso base):\")\n",
        "dt = datamerge(datasets[0],datasets[0],'entity','label')\n",
        "print(dt)\n",
        "jaccard_simples(dt['doc_freq'].tolist())\n",
        "print(\"\\nComparando documentos 0 e 1:\")\n",
        "dt = datamerge(datasets[0],datasets[1],'entity','label')\n",
        "print(dt)\n",
        "jaccard_simples(dt['doc_freq'].tolist())\n",
        "print(\"\\nComparando documentos 0 e 2:\")\n",
        "dt = datamerge(datasets[0],datasets[2],'entity','label')\n",
        "print(dt)\n",
        "jaccard_simples(dt['doc_freq'].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zw49iuWWb-m",
        "outputId": "e82050de-9558-4930-a238-a3a449b3a784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparando documento 0 consigo mesmo (caso base):\n",
            "   entity   label  freq_x  freq_y  doc_freq\n",
            "0  Airbnb     GPE       1       1         2\n",
            "2  Airbnb  PERSON       1       1         2\n",
            "1   China     GPE       2       2         2\n",
            "interseção: 3\n",
            "união: 3\n",
            "jaccard simplificada: 1.0\n",
            "\n",
            "Comparando documentos 0 e 1:\n",
            "   entity   label  freq_x  freq_y  doc_freq\n",
            "0  Airbnb     GPE       1     0.0         1\n",
            "2  Airbnb  PERSON       1     1.0         2\n",
            "1   China     GPE       2     1.0         2\n",
            "interseção: 2\n",
            "união: 3\n",
            "jaccard simplificada: 0.6666666666666666\n",
            "\n",
            "Comparando documentos 0 e 2:\n",
            "         entity   label  freq_x  freq_y  doc_freq\n",
            "0        Airbnb     GPE     1.0     0.0         1\n",
            "2        Airbnb  PERSON     1.0     0.0         1\n",
            "3         Apple     GPE     0.0     1.0         1\n",
            "1         China     GPE     2.0     0.0         1\n",
            "4  Estee Lauder  PERSON     0.0     1.0         1\n",
            "interseção: 0\n",
            "união: 5\n",
            "jaccard simplificada: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similaridade_WP_means(str0,str1):\n",
        "  means = []\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se synset0 contém alguma definição\n",
        "    if len(syn0) > 0:\n",
        "      val = 0\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se synset1 contém alguma definição\n",
        "        if len(syn1) > 0:\n",
        "          val+= syn0[0].wup_similarity(syn1[0])\n",
        "          #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"val:\",val)\n",
        "      means.append(val/len(str1))\n",
        "  print(means)\n",
        "  print(sum(means)/len(means))\n",
        "\n",
        "def similaridade_WP_max(str0,str1):\n",
        "  means = []\n",
        "  vals = []\n",
        "  aux = 0\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se 1a palavra contém alguma definição antes de prosseguir\n",
        "    if len(syn0) != 0:\n",
        "      vals.clear()\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se 2a palavra contém alguma definição antes de calcular similaridade\n",
        "        if len(syn1) != 0:\n",
        "          aux = syn0[0].wup_similarity(syn1[0])\n",
        "          # se similaridade = 1, palavras iguais, break\n",
        "          if aux == 1:\n",
        "            break\n",
        "          # senão, adiciona valor à lista vals\n",
        "          else:\n",
        "            vals.append(aux)\n",
        "            #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"wup:\",syn0[0].wup_similarity(syn1[0]))\n",
        "      # se aux = 1, similaridade máxima = 1\n",
        "      if aux == 1:\n",
        "         means.append(aux)\n",
        "      # senão seleciona máxima da lista\n",
        "      else:\n",
        "        means.append(max(vals))\n",
        "  print(means)\n",
        "  print(sum(means)/len(means))\n",
        "\n",
        "str0 = filtertuples(preprocess(urls[0]),True)\n",
        "str1 = filtertuples(preprocess(urls[1]),True)\n",
        "str2 = filtertuples(preprocess(urls[2]),True)\n",
        "\n",
        "print(\"string 0:\",str0['word'].tolist())\n",
        "print(\"string 1:\",str1['word'].tolist())\n",
        "similaridade_WP_means(str0['word'].tolist(),str1['word'].tolist())\n",
        "similaridade_WP_max(str0['word'].tolist(),str1['word'].tolist())\n",
        "dt = datamerge(str0,str1,'word','pos_tag')\n",
        "print(dt)\n",
        "jaccard_simples(dt['doc_freq'].tolist())\n",
        "\n",
        "print('\\n')\n",
        "print(\"string 0:\",str0['word'].tolist())\n",
        "print(\"string 2:\",str2['word'].tolist())\n",
        "similaridade_WP_means(str0['word'].tolist(),str2['word'].tolist())\n",
        "similaridade_WP_max(str0['word'].tolist(),str2['word'].tolist())\n",
        "dt = datamerge(str0,str2,'word','pos_tag')\n",
        "print(dt)\n",
        "jaccard_simples(dt['doc_freq'].tolist())\n",
        "\n",
        "# cosine do scikit\n",
        "print('\\ncosine do scikit')\n",
        "corpus = [urls[0],urls[1]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "corpus = [urls[0],urls[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "corpus = [urls[0],urls[1],urls[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVY6_bkXrBr_",
        "outputId": "cc575a0f-a5ce-4804-a5f7-26343a0e1c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "string 0: ['Airbnb', 'shut', 'listings', 'China', 'two', 'years', 'lockdowns', 'country']\n",
            "string 1: ['Starting', 'summer', 'Airbnb', 'take', 'listings', 'offers', 'hosted', 'experiences', 'China']\n",
            "[0.17105672105672104, 0.29106753812636166, 0.1999273783587509, 0.21296296296296294, 0.25932150638032986, 0.26241529750301684, 0.19814814814814813]\n",
            "0.22784279321947018\n",
            "[0.25, 1.0, 1.0, 0.4, 0.7142857142857143, 0.631578947368421, 0.26666666666666666]\n",
            "0.6089330469029717\n",
            "           word pos_tag  freq_x  freq_y  doc_freq\n",
            "0        Airbnb     NNP     2.0     1.0         2\n",
            "3         China     NNP     2.0     1.0         2\n",
            "8      Starting     VBG     0.0     1.0         1\n",
            "7       country      NN     1.0     0.0         1\n",
            "13  experiences     NNS     0.0     1.0         1\n",
            "12       hosted      JJ     0.0     1.0         1\n",
            "2      listings     NNS     2.0     1.0         2\n",
            "6     lockdowns     NNS     1.0     0.0         1\n",
            "11       offers     NNS     0.0     1.0         1\n",
            "1          shut      VB     2.0     0.0         1\n",
            "9        summer      NN     0.0     1.0         1\n",
            "10         take      VB     0.0     1.0         1\n",
            "4           two      CD     1.0     0.0         1\n",
            "5         years     NNS     1.0     0.0         1\n",
            "interseção: 3\n",
            "união: 14\n",
            "jaccard simplificada: 0.21428571428571427\n",
            "\n",
            "\n",
            "string 0: ['Airbnb', 'shut', 'listings', 'China', 'two', 'years', 'lockdowns', 'country']\n",
            "string 2: ['International', 'brands', 'Apple', 'Estee', 'Lauder', 'have', 'warned', 'financial', 'impact', 'restrictions']\n",
            "[0.20388888888888887, 0.18476190476190474, 0.16702548225768038, 0.172291923762512, 0.18476190476190474, 0.17561612364243942, 0.18405662964486494]\n",
            "0.1817718368171707\n",
            "[0.5, 0.2857142857142857, 0.35294117647058826, 0.26666666666666666, 0.2857142857142857, 0.4444444444444444, 0.35294117647058826]\n",
            "0.35548886221155124\n",
            "             word pos_tag  freq_x  freq_y  doc_freq\n",
            "0          Airbnb     NNP     2.0     0.0         1\n",
            "10          Apple     NNP     0.0     1.0         1\n",
            "3           China     NNP     2.0     0.0         1\n",
            "11          Estee     NNP     0.0     1.0         1\n",
            "8   International     NNP     0.0     1.0         1\n",
            "12         Lauder     NNP     0.0     1.0         1\n",
            "9          brands     NNS     0.0     1.0         1\n",
            "7         country      NN     1.0     0.0         1\n",
            "15      financial      JJ     0.0     1.0         1\n",
            "13           have     VBP     0.0     1.0         1\n",
            "16         impact      NN     0.0     1.0         1\n",
            "2        listings     NNS     2.0     0.0         1\n",
            "6       lockdowns     NNS     1.0     0.0         1\n",
            "17   restrictions     NNS     0.0     1.0         1\n",
            "1            shut      VB     2.0     0.0         1\n",
            "4             two      CD     1.0     0.0         1\n",
            "14         warned     VBN     0.0     1.0         1\n",
            "5           years     NNS     1.0     0.0         1\n",
            "interseção: 0\n",
            "união: 18\n",
            "jaccard simplificada: 0.0\n",
            "\n",
            "cosine do scikit\n",
            "[[1.         0.40733417]]\n",
            "[[1.         0.07707006]]\n",
            "[[1.         0.44413568 0.10492044]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getclasses(string):\n",
        "  lemmas = []\n",
        "  hypernyms = []\n",
        "  na = 'N\\A'\n",
        "  for wd in string:\n",
        "    syn = wn.synsets(wd)\n",
        "    if len(syn) > 0:\n",
        "      lemmas.append(syn[0].lemmas()[0].name())\n",
        "      if len(syn[0].hypernyms()) > 0:\n",
        "        hypernyms.append(syn[0].hypernyms()[0].name())\n",
        "      else:\n",
        "        hypernyms.append(na)\n",
        "    else:\n",
        "      lemmas.append(na)\n",
        "      hypernyms.append(na)\n",
        "  return pd.DataFrame(zip(string,lemmas,hypernyms),columns =['word','lemma','hypernym'])\n",
        "\n",
        "def intersects(df0,df1,col):\n",
        "  aux = []\n",
        "  for i in df0.tolist():\n",
        "    if i != 'N\\A':\n",
        "      aux.append(i)\n",
        "  df0 = pd.DataFrame(zip(aux),columns =[col])\n",
        "  df0['freq'] = df0.groupby([col])[col].transform('count')\n",
        "  print(df0)\n",
        "  aux.clear()\n",
        "  for i in df1.tolist():\n",
        "    if i != 'N\\A':\n",
        "      aux.append(i)\n",
        "  df1 = pd.DataFrame(zip(aux),columns =[col])\n",
        "  df1['freq'] = df1.groupby([col])[col].transform('count')\n",
        "  print(df1)\n",
        "  merged = pd.merge(df0,df1, on=[col], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  return merged\n",
        "\n",
        "print(preprocess(urls[0]))\n",
        "print(filtertuples(preprocess(urls[0]),False))\n",
        "str0 = getclasses(filtertuples(preprocess(urls[0]),False))\n",
        "print(str0,'\\n')\n",
        "\n",
        "print(preprocess(urls[1]))\n",
        "print(filtertuples(preprocess(urls[1]),False))\n",
        "str1 = getclasses(filtertuples(preprocess(urls[1]),False))\n",
        "print(str1,'\\n')\n",
        "\n",
        "print(intersects(str0['lemma'],str1['lemma'],'lemma'))\n",
        "print(intersects(str0['hypernym'],str1['hypernym'],'hypernym'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkon-CDeW5bt",
        "outputId": "339695eb-3df3-4011-960b-c9fc56276beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Airbnb', 'NNP'), ('will', 'MD'), ('shut', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('in', 'IN'), ('China', 'NNP'), ('after', 'IN'), ('two', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('lockdowns', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('country', 'NN'), ('Airbnb', 'NNP'), ('will', 'MD'), ('shut', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('in', 'IN'), ('China', 'NNP')]\n",
            "['Airbnb', 'shut', 'listings', 'China', 'two', 'years', 'lockdowns', 'country', 'Airbnb', 'shut', 'listings', 'China']\n",
            "         word     lemma             hypernym\n",
            "0      Airbnb       N\\A                  N\\A\n",
            "1        shut     close                  N\\A\n",
            "2    listings      list        database.n.01\n",
            "3       China     China                  N\\A\n",
            "4         two       two           digit.n.01\n",
            "5       years   old_age    time_of_life.n.01\n",
            "6   lockdowns  lockdown    imprisonment.n.03\n",
            "7     country     state  political_unit.n.01\n",
            "8      Airbnb       N\\A                  N\\A\n",
            "9        shut     close                  N\\A\n",
            "10   listings      list        database.n.01\n",
            "11      China     China                  N\\A \n",
            "\n",
            "[('Starting', 'VBG'), ('this', 'DT'), ('summer', 'NN'), ('Airbnb', 'NNP'), ('will', 'MD'), ('take', 'VB'), ('down', 'RP'), ('its', 'PRP$'), ('listings', 'NNS'), ('and', 'CC'), ('offers', 'NNS'), ('for', 'IN'), ('hosted', 'JJ'), ('experiences', 'NNS'), ('in', 'IN'), ('China', 'NNP')]\n",
            "['Starting', 'summer', 'Airbnb', 'take', 'listings', 'offers', 'hosted', 'experiences', 'China']\n",
            "          word       lemma         hypernym\n",
            "0     Starting       start        turn.n.03\n",
            "1       summer      summer      season.n.02\n",
            "2       Airbnb         N\\A              N\\A\n",
            "3         take      return      income.n.01\n",
            "4     listings        list    database.n.01\n",
            "5       offers       offer  speech_act.n.01\n",
            "6       hosted        host   entertain.v.01\n",
            "7  experiences  experience   education.n.02\n",
            "8        China       China              N\\A \n",
            "\n",
            "      lemma  freq\n",
            "0     close     2\n",
            "1      list     2\n",
            "2     China     2\n",
            "3       two     1\n",
            "4   old_age     1\n",
            "5  lockdown     1\n",
            "6     state     1\n",
            "7     close     2\n",
            "8      list     2\n",
            "9     China     2\n",
            "        lemma  freq\n",
            "0       start     1\n",
            "1      summer     1\n",
            "2      return     1\n",
            "3        list     1\n",
            "4       offer     1\n",
            "5        host     1\n",
            "6  experience     1\n",
            "7       China     1\n",
            "         lemma  freq_x  freq_y\n",
            "0        close     2.0     0.0\n",
            "2         list     2.0     1.0\n",
            "4        China     2.0     1.0\n",
            "6          two     1.0     0.0\n",
            "7      old_age     1.0     0.0\n",
            "8     lockdown     1.0     0.0\n",
            "9        state     1.0     0.0\n",
            "10       start     0.0     1.0\n",
            "11      summer     0.0     1.0\n",
            "12      return     0.0     1.0\n",
            "13       offer     0.0     1.0\n",
            "14        host     0.0     1.0\n",
            "15  experience     0.0     1.0\n",
            "              hypernym  freq\n",
            "0        database.n.01     2\n",
            "1           digit.n.01     1\n",
            "2    time_of_life.n.01     1\n",
            "3    imprisonment.n.03     1\n",
            "4  political_unit.n.01     1\n",
            "5        database.n.01     2\n",
            "          hypernym  freq\n",
            "0        turn.n.03     1\n",
            "1      season.n.02     1\n",
            "2      income.n.01     1\n",
            "3    database.n.01     1\n",
            "4  speech_act.n.01     1\n",
            "5   entertain.v.01     1\n",
            "6   education.n.02     1\n",
            "               hypernym  freq_x  freq_y\n",
            "0         database.n.01     2.0     1.0\n",
            "2            digit.n.01     1.0     0.0\n",
            "3     time_of_life.n.01     1.0     0.0\n",
            "4     imprisonment.n.03     1.0     0.0\n",
            "5   political_unit.n.01     1.0     0.0\n",
            "6             turn.n.03     0.0     1.0\n",
            "7           season.n.02     0.0     1.0\n",
            "8           income.n.01     0.0     1.0\n",
            "9       speech_act.n.01     0.0     1.0\n",
            "10       entertain.v.01     0.0     1.0\n",
            "11       education.n.02     0.0     1.0\n"
          ]
        }
      ]
    }
  ]
}