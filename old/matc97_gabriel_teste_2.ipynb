{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL2bUWe1VNReOweOWSoTgQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielFCosta/TCC-BSI-2023.1/blob/main/old/matc97_gabriel_teste_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g1zLg3Ipx77",
        "outputId": "29cb1e7a-7741-4be0-9aa0-63554842f0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import time as tm\n",
        "from urllib.request import urlopen\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWcc_InWq1HI",
        "outputId": "01896eac-c995-4d9e-ecb6-6f7f601017dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove pontuação, utilizada pela função seguinte de pré-processamento\n",
        "def remover_pontuacao(tokens):\n",
        "  for token in tokens:\n",
        "    if token in string.punctuation:\n",
        "      tokens.remove(token)\n",
        "# Função de pré-processamento\n",
        "def preprocess(text):\n",
        "    text = word_tokenize(text)\n",
        "    remover_pontuacao(text)\n",
        "    return pos_tag(text)\n",
        "\n",
        "# Listas para coletar rótulos e entidades (palavras) do texto\n",
        "codes = []\n",
        "words = []\n",
        "def  clearlists():\n",
        "  if len(words) > 0:\n",
        "    words.clear()\n",
        "  if len(codes) > 0:\n",
        "    codes.clear()\n",
        "\n",
        "# Função de reconhecimento de entidades nomeadas (NER)\n",
        "def ner(sent):\n",
        "  ne_tree = nltk.ne_chunk(sent)\n",
        "  for chunk in ne_tree:\n",
        "    if hasattr(chunk,\"label\"):\n",
        "      word =' '.join(c[0] for c in chunk)\n",
        "      #print(chunk.label(),word)\n",
        "      codes.append(chunk.label())\n",
        "      words.append(word)\n",
        "\n",
        "# Função que retorna dataframe de entidades (palavras), rótulos e frequências \n",
        "def returndataset(col1,col2):\n",
        "  # junta listas de palavras e rótulos num dataframe\n",
        "  dataset = pd.DataFrame(zip(words,codes),columns =[col1,col2])\n",
        "  clearlists()\n",
        "  # calcula frequências das palavras e adiciona nova coluna\n",
        "  dataset['freq'] = dataset.groupby([col1,col2])[col1].transform('count')\n",
        "  return dataclean(dataset)\n",
        " \n",
        "# Remove linhas duplicatas e atualiza index\n",
        "def dataclean(dataset):\n",
        "  dataset.drop_duplicates(inplace= True)\n",
        "  dataset.reset_index()\n",
        "  return dataset\n",
        "\n",
        "# Verifica rótulo gramatical\n",
        "def test(tag):\n",
        "  # verificando todos substantivos, adjetivos, verbos e numerais cardinais.\n",
        "  #tags = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ','CD']\n",
        "  tags = ['NN','NNS','NNP','NNPS']\n",
        "  for i in tags:\n",
        "    if i == tag:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "# Filtra palavras da lista gerada pelo pos tagging e retorna dataframe\n",
        "def filtertuples(textlist,rtdata):\n",
        "  for tuple in textlist:\n",
        "    if test(tuple[1]):\n",
        "      words.append(tuple[0])\n",
        "      codes.append(tuple[1])\n",
        "  if rtdata:\n",
        "    return returndataset('word','pos_tag')\n",
        "  else:\n",
        "    aux = words.copy()\n",
        "    clearlists()\n",
        "  return aux\n",
        "\n",
        "# Retorna união de dois dataframes com \"frequência de documentos\" \n",
        "def datamerge(words0,words1,col1,col2):\n",
        "  merged = pd.merge(words0,words1, on=[col1,col2], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  merged['doc_freq'] = getdoc_freq(merged)\n",
        "  return merged.sort_values(by=[col1,col2])\n",
        "\n",
        "# Retorna lista de \"frequência de documentos\"\n",
        "def getdoc_freq(merged):\n",
        "  doc_freq = []\n",
        "  c = 0\n",
        "  # verifica duas colunas de frequência\n",
        "  for i in merged['freq_x'].tolist():\n",
        "    # se frequência maior que 0 em ambas colunas \n",
        "    if i > 0 and merged['freq_y'].tolist()[c] > 0:\n",
        "      doc_freq.append(2)\n",
        "    else:\n",
        "      doc_freq.append(1)\n",
        "    c += 1\n",
        "  return doc_freq\n",
        "\n",
        "# interseção dividida pela união sem considerar frequências\n",
        "def jaccard_simples(doc_freq):\n",
        "    intersecao = 0\n",
        "    c = 0\n",
        "    for x in doc_freq:\n",
        "      if x == 2:\n",
        "        intersecao += 1\n",
        "      c += 1\n",
        "    #print(\"interseção:\",intersecao)\n",
        "    uniao = len(doc_freq)\n",
        "    #print(\"união:\",uniao)\n",
        "    print(\"jaccard_simples:\",intersecao/uniao)\n",
        "\n",
        "# Calculado a partir das frequências\n",
        "# De acordo com https://towardsdatascience.com/nlp-text-similarity-how-it-works-and-the-math-behind-it-a0fb90a05095\n",
        "def cosine_simples(mergedf):\n",
        "  numerador = 0\n",
        "  vetx = 0\n",
        "  vety = 0\n",
        "  for idx, row in mergedf.iterrows():\n",
        "    numerador += row['freq_x'] * row['freq_y']\n",
        "    vetx += pow(row['freq_x'],2)\n",
        "    vety += pow(row['freq_y'],2)\n",
        "  vetx = math.sqrt(vetx)\n",
        "  vety = math.sqrt(vety)\n",
        "  denominador = vetx * vety\n",
        "  print(\"cosine_simples:\",numerador/denominador)\n",
        "  "
      ],
      "metadata": {
        "id": "e1C1u6qgrWtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similaridade com base na média das médias. Não utilizar, caso base != 1\n",
        "def similaridade_WP_means(str0,str1):\n",
        "  means = []\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se synset0 contém alguma definição\n",
        "    if len(syn0) > 0:\n",
        "      val = 0\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se synset1 contém alguma definição\n",
        "        if len(syn1) > 0:\n",
        "          val+= syn0[0].wup_similarity(syn1[0])\n",
        "          #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"val:\",val)\n",
        "      means.append(val/len(str1))\n",
        "  #print(means)\n",
        "  print('média das médias WP:',sum(means)/len(means))\n",
        "\n",
        "# Similaridade com base na média das similaridades máximas entre palavras\n",
        "def similaridade_WP_max(str0,str1):\n",
        "  means = []\n",
        "  vals = []\n",
        "  aux = 0\n",
        "  for wd0 in str0:\n",
        "    syn0 = wn.synsets(wd0)\n",
        "    # verifica se 1a palavra contém alguma definição antes de prosseguir\n",
        "    if len(syn0) != 0:\n",
        "      vals.clear()\n",
        "      for wd1 in str1:\n",
        "        syn1 = wn.synsets(wd1)\n",
        "        # verifica se 2a palavra contém alguma definição antes de calcular similaridade\n",
        "        if len(syn1) != 0:\n",
        "          aux = syn0[0].wup_similarity(syn1[0])\n",
        "          # se similaridade = 1, palavras iguais, break\n",
        "          if aux == 1:\n",
        "            break\n",
        "          # senão, adiciona valor à lista vals\n",
        "          else:\n",
        "            vals.append(aux)\n",
        "            #print(\"syn0:\",syn0[0],\"syn1:\",syn1[0],\"wup:\",syn0[0].wup_similarity(syn1[0]))\n",
        "      # se aux = 1, similaridade máxima = 1\n",
        "      if aux == 1:\n",
        "         means.append(aux)\n",
        "      # senão seleciona máxima da lista\n",
        "      else:\n",
        "        means.append(max(vals))\n",
        "  #print(means)\n",
        "  print('média das máximas WP:',sum(means)/len(means))\n"
      ],
      "metadata": {
        "id": "hVY6_bkXrBr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retorna lemmas e hiperônimos das palavras contidas na lista do parâmetro\n",
        "def getclasses(stringlist):\n",
        "  lemmas = []\n",
        "  hypernyms = []\n",
        "  na = 'N\\A'\n",
        "  for wd in stringlist:\n",
        "    syn = wn.synsets(wd)\n",
        "    if len(syn) > 0:\n",
        "      lemmas.append(syn[0].lemmas()[0].name())\n",
        "      if len(syn[0].hypernyms()) > 0:\n",
        "        hypernyms.append(syn[0].hypernyms()[0].name())\n",
        "      else:\n",
        "        hypernyms.append(na)\n",
        "    else:\n",
        "      lemmas.append(na)\n",
        "      hypernyms.append(na)\n",
        "  return pd.DataFrame(zip(stringlist,lemmas,hypernyms),columns =['word','lemma','hypernym'])\n",
        "\n",
        "# Retorna tabela de frequências do tipo de classe (lemma ou hypernym), passado no parâmetro col\n",
        "def intersects(df0,df1,col):\n",
        "  aux = []\n",
        "  for i in df0.tolist():\n",
        "    if i != 'N\\A':\n",
        "      aux.append(i)\n",
        "  df0 = pd.DataFrame(zip(aux),columns =[col])\n",
        "  df0['freq'] = df0.groupby([col])[col].transform('count')\n",
        "  #print(df0)\n",
        "  aux.clear()\n",
        "  for i in df1.tolist():\n",
        "    if i != 'N\\A':\n",
        "      aux.append(i)\n",
        "  df1 = pd.DataFrame(zip(aux),columns =[col])\n",
        "  df1['freq'] = df1.groupby([col])[col].transform('count')\n",
        "  #print(df1)\n",
        "  return mergeclass(df0,df1,col)\n",
        "\n",
        "# Auxiliar à função intersects pra fazer o merge das tabelas do parâmetro\n",
        "def mergeclass(df0,df1,col):\n",
        "  merged = pd.merge(df0,df1, on=[col], how='outer')\n",
        "  merged = dataclean(merged).fillna(0)\n",
        "  merged['doc_freq'] = getdoc_freq(merged)\n",
        "  return merged.sort_values(by=[col])\n"
      ],
      "metadata": {
        "id": "nkon-CDeW5bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execução do reconhecimento de entidades nomeadadas:\n",
        "'''\n",
        "urls = [\"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia.txt\",\n",
        "        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia1.txt\",\n",
        "        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/noticia2.txt\"]\n",
        "# Lista que vai conter dataframes de entidades\n",
        "datasets = []\n",
        "\n",
        "# Pra cada url\n",
        "for url in urls:\n",
        "  data = urlopen(url)\n",
        "  ex = data.read().decode('utf-8')\n",
        "  ex = nltk.sent_tokenize(url)\n",
        "  print(ex)\n",
        "  # Detectando entidades de nomeadas por sentença\n",
        "  for sentence in ex:\n",
        "    # Sentença Pré-processada\n",
        "    text = preprocess(sentence)\n",
        "    print(text)\n",
        "    # Entidades Nomeadas na Sentença\n",
        "    ner(text)\n",
        "  # Adiciona dataframe de entidades nomeadas do texto + labels, frequências e TF\n",
        "  datasets.append(returndataset('entity','label'))\n",
        "\n",
        "c = 0\n",
        "for dataset in datasets:\n",
        "  print(\"\\nstring:\",c)\n",
        "  print(dataset)\n",
        "  c += 1\n",
        "'''\n",
        "frases = [\"Airbnb will shut down its listings in China after two years of lockdowns in the country.\",\n",
        "        \"Starting this summer, Airbnb will take down its listings and offers for hosted experiences in China.\",\n",
        "        \"International brands, from Apple to Estee Lauder, have warned of the financial impact of the restrictions.\"]\n",
        "\n",
        "def compnomeadas(frase0,frase1):\n",
        "  ner(preprocess(frase0))\n",
        "  dt0 = returndataset('entity','label')\n",
        "  ner(preprocess(frase1))\n",
        "  dt = datamerge(dt0,returndataset('entity','label'),'entity','label')\n",
        "  #print(dt)\n",
        "  jaccard_simples(dt['doc_freq'].tolist())\n",
        "  cosine_simples(dt)\n",
        "\n",
        "def comppostags(frase0,frase1):\n",
        "  str0 = filtertuples(preprocess(frase0),True)\n",
        "  str1 = filtertuples(preprocess(frase1),True)\n",
        "  #print(\"string 0:\",str0['word'].tolist())\n",
        "  #print(\"string 1:\",str1['word'].tolist())\n",
        "  similaridade_WP_max(str0['word'].tolist(),str1['word'].tolist())\n",
        "  dt = datamerge(str0,str1,'word','pos_tag')\n",
        "  jaccard_simples(dt['doc_freq'].tolist())\n",
        "  cosine_simples(dt)\n",
        "\n",
        "def compwordnet(frase0,frase1,classe):\n",
        "  #print(preprocess(frase0))\n",
        "  print(filtertuples(preprocess(frase0),False))\n",
        "  str0 = getclasses(filtertuples(preprocess(frase0),False))\n",
        "  print(str0,'\\n')\n",
        "  str1 = getclasses(filtertuples(preprocess(frase1),False))\n",
        "  print(str1,'\\n')\n",
        "  df = intersects(str0[classe],str1[classe],classe)\n",
        "  print(df)\n",
        "  jaccard_simples(df['doc_freq'].tolist())\n",
        "  cosine_simples(df)"
      ],
      "metadata": {
        "id": "3qeGsrP5hV1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = tm.time()\n",
        "print(\"\\n######## Comparação de Entidades Nomeadas ########\")\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "compnomeadas(frases[0],frases[0])\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "compnomeadas(frases[0],frases[1])\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "compnomeadas(frases[0],frases[2])\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "start = tm.time()\n",
        "print(\"\\n######## Comparação de POS TAGS ########\")\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "comppostags(frases[0],frases[0])\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "comppostags(frases[0],frases[1])\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "comppostags(frases[0],frases[2])\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "# cosine do scikit\n",
        "start = tm.time()\n",
        "print('\\n######## Cosine do Scikit ########')\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "corpus = [frases[0],frases[1]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "corpus = [frases[0],frases[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "print(\"\\n## strings 0, 1 e 2: ##\")\n",
        "corpus = [frases[0],frases[1],frases[2]]\n",
        "vectorizer = TfidfVectorizer()\n",
        "trsfm=vectorizer.fit_transform(corpus)\n",
        "print(cosine_similarity(trsfm[0:1], trsfm))\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "start = tm.time()\n",
        "print('\\n######## Comparação de Lemmas do Wordnet ########')\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "compwordnet(frases[0],frases[0],'lemma')\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "compwordnet(frases[0],frases[1],'lemma')\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "compwordnet(frases[0],frases[2],'lemma')\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n",
        "\n",
        "start = tm.time()\n",
        "print('\\n######## Comparação de Hiperônimos do Wordnet ########')\n",
        "print(\"\\n## strings 0 e 0 (caso base): ##\")\n",
        "compwordnet(frases[0],frases[0],'hypernym')\n",
        "print(\"\\n## strings 0 e 1: ##\")\n",
        "compwordnet(frases[0],frases[1],'hypernym')\n",
        "print(\"\\n## strings 0 e 2: ##\")\n",
        "compwordnet(frases[0],frases[2],'hypernym')\n",
        "end = tm.time()\n",
        "print(\"\\n######## Tempo de execução:\", end-start,\"########\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zw49iuWWb-m",
        "outputId": "19a638b8-d205-4155-ce0d-7e9c9c6854b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######## Comparação de Entidades Nomeadas ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "jaccard_simples: 1.0\n",
            "cosine_simples: 0.9999999999999998\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "jaccard_simples: 0.3333333333333333\n",
            "cosine_simples: 0.4999999999999999\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "jaccard_simples: 0.0\n",
            "cosine_simples: 0.0\n",
            "\n",
            "######## Tempo de execução: 0.38097310066223145 ########\n",
            "\n",
            "######## Comparação de POS TAGS ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "média das máximas WP: 1.0\n",
            "jaccard_simples: 1.0\n",
            "cosine_simples: 1.0000000000000002\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "média das máximas WP: 0.7073015873015873\n",
            "jaccard_simples: 0.3333333333333333\n",
            "cosine_simples: 0.5000000000000001\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "média das máximas WP: 0.3337628384687208\n",
            "jaccard_simples: 0.0\n",
            "cosine_simples: 0.0\n",
            "\n",
            "######## Tempo de execução: 2.3257617950439453 ########\n",
            "\n",
            "######## Cosine do Scikit ########\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "[[1.         0.31639145]]\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "[[1.         0.12254598]]\n",
            "\n",
            "## strings 0, 1 e 2: ##\n",
            "[[1.         0.35608377 0.15772405]]\n",
            "\n",
            "######## Tempo de execução: 0.0280611515045166 ########\n",
            "\n",
            "######## Comparação de Lemmas do Wordnet ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "['Airbnb', 'listings', 'China', 'years', 'lockdowns', 'country']\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "      lemma  freq_x  freq_y  doc_freq\n",
            "1     China       1       1         2\n",
            "0      list       1       1         2\n",
            "3  lockdown       1       1         2\n",
            "2   old_age       1       1         2\n",
            "4     state       1       1         2\n",
            "jaccard_simples: 1.0\n",
            "cosine_simples: 0.9999999999999998\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "['Airbnb', 'listings', 'China', 'years', 'lockdowns', 'country']\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "          word       lemma         hypernym\n",
            "0       summer      summer      season.n.02\n",
            "1       Airbnb         N\\A              N\\A\n",
            "2     listings        list    database.n.01\n",
            "3       offers       offer  speech_act.n.01\n",
            "4  experiences  experience   education.n.02\n",
            "5        China       China              N\\A \n",
            "\n",
            "        lemma  freq_x  freq_y  doc_freq\n",
            "1       China     1.0     1.0         2\n",
            "7  experience     0.0     1.0         1\n",
            "0        list     1.0     1.0         2\n",
            "3    lockdown     1.0     0.0         1\n",
            "6       offer     0.0     1.0         1\n",
            "2     old_age     1.0     0.0         1\n",
            "4       state     1.0     0.0         1\n",
            "5      summer     0.0     1.0         1\n",
            "jaccard_simples: 0.25\n",
            "cosine_simples: 0.3999999999999999\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "['Airbnb', 'listings', 'China', 'years', 'lockdowns', 'country']\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "            word          lemma           hypernym\n",
            "0  International  International     socialism.n.02\n",
            "1         brands     trade_name          name.n.01\n",
            "2          Apple          apple  edible_fruit.n.01\n",
            "3          Estee            N\\A                N\\A\n",
            "4         Lauder         Lauder                N\\A\n",
            "5         impact         impact       contact.n.04\n",
            "6   restrictions    restriction          rule.n.01 \n",
            "\n",
            "            lemma  freq_x  freq_y  doc_freq\n",
            "1           China     1.0     0.0         1\n",
            "5   International     0.0     1.0         1\n",
            "8          Lauder     0.0     1.0         1\n",
            "7           apple     0.0     1.0         1\n",
            "9          impact     0.0     1.0         1\n",
            "0            list     1.0     0.0         1\n",
            "3        lockdown     1.0     0.0         1\n",
            "2         old_age     1.0     0.0         1\n",
            "10    restriction     0.0     1.0         1\n",
            "4           state     1.0     0.0         1\n",
            "6      trade_name     0.0     1.0         1\n",
            "jaccard_simples: 0.0\n",
            "cosine_simples: 0.0\n",
            "\n",
            "######## Tempo de execução: 0.08811283111572266 ########\n",
            "\n",
            "######## Comparação de Hiperônimos do Wordnet ########\n",
            "\n",
            "## strings 0 e 0 (caso base): ##\n",
            "['Airbnb', 'listings', 'China', 'years', 'lockdowns', 'country']\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "              hypernym  freq_x  freq_y  doc_freq\n",
            "0        database.n.01       1       1         2\n",
            "2    imprisonment.n.03       1       1         2\n",
            "3  political_unit.n.01       1       1         2\n",
            "1    time_of_life.n.01       1       1         2\n",
            "jaccard_simples: 1.0\n",
            "cosine_simples: 1.0\n",
            "\n",
            "## strings 0 e 1: ##\n",
            "['Airbnb', 'listings', 'China', 'years', 'lockdowns', 'country']\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "          word       lemma         hypernym\n",
            "0       summer      summer      season.n.02\n",
            "1       Airbnb         N\\A              N\\A\n",
            "2     listings        list    database.n.01\n",
            "3       offers       offer  speech_act.n.01\n",
            "4  experiences  experience   education.n.02\n",
            "5        China       China              N\\A \n",
            "\n",
            "              hypernym  freq_x  freq_y  doc_freq\n",
            "0        database.n.01     1.0     1.0         2\n",
            "6       education.n.02     0.0     1.0         1\n",
            "2    imprisonment.n.03     1.0     0.0         1\n",
            "3  political_unit.n.01     1.0     0.0         1\n",
            "4          season.n.02     0.0     1.0         1\n",
            "5      speech_act.n.01     0.0     1.0         1\n",
            "1    time_of_life.n.01     1.0     0.0         1\n",
            "jaccard_simples: 0.14285714285714285\n",
            "cosine_simples: 0.25\n",
            "\n",
            "## strings 0 e 2: ##\n",
            "['Airbnb', 'listings', 'China', 'years', 'lockdowns', 'country']\n",
            "        word     lemma             hypernym\n",
            "0     Airbnb       N\\A                  N\\A\n",
            "1   listings      list        database.n.01\n",
            "2      China     China                  N\\A\n",
            "3      years   old_age    time_of_life.n.01\n",
            "4  lockdowns  lockdown    imprisonment.n.03\n",
            "5    country     state  political_unit.n.01 \n",
            "\n",
            "            word          lemma           hypernym\n",
            "0  International  International     socialism.n.02\n",
            "1         brands     trade_name          name.n.01\n",
            "2          Apple          apple  edible_fruit.n.01\n",
            "3          Estee            N\\A                N\\A\n",
            "4         Lauder         Lauder                N\\A\n",
            "5         impact         impact       contact.n.04\n",
            "6   restrictions    restriction          rule.n.01 \n",
            "\n",
            "              hypernym  freq_x  freq_y  doc_freq\n",
            "7         contact.n.04     0.0     1.0         1\n",
            "0        database.n.01     1.0     0.0         1\n",
            "6    edible_fruit.n.01     0.0     1.0         1\n",
            "2    imprisonment.n.03     1.0     0.0         1\n",
            "5            name.n.01     0.0     1.0         1\n",
            "3  political_unit.n.01     1.0     0.0         1\n",
            "8            rule.n.01     0.0     1.0         1\n",
            "4       socialism.n.02     0.0     1.0         1\n",
            "1    time_of_life.n.01     1.0     0.0         1\n",
            "jaccard_simples: 0.0\n",
            "cosine_simples: 0.0\n",
            "\n",
            "######## Tempo de execução: 0.0915374755859375 ########\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retorna todos elementos da string numa lista\n",
        "def retstringlist(string):\n",
        "  textlist = preprocess(string)\n",
        "  aux = []\n",
        "  for tuple in textlist:\n",
        "    aux.append(tuple[0])\n",
        "  return aux\n",
        "# converte lista pra dataframe de frequências\n",
        "def returndatasimples(aux):\n",
        "  # junta listas de palavras e rótulos num dataframe\n",
        "  dataset = pd.DataFrame(aux,columns =['elementos'])\n",
        "  # calcula frequências das palavras e adiciona nova coluna\n",
        "  dataset['freq'] = dataset.groupby(['elementos'])['elementos'].transform('count')\n",
        "  return dataclean(dataset)\n",
        "\n",
        "df0 = returndatasimples(retstringlist(frases[0]))\n",
        "df1 = returndatasimples(retstringlist(frases[1]))\n",
        "df2 = returndatasimples(retstringlist(frases[2]))\n",
        "cosine_simples(mergeclass(df0,df0,'elementos'))\n",
        "cosine_simples(mergeclass(df0,df1,'elementos'))\n",
        "cosine_simples(mergeclass(df0,df2,'elementos'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2HaYIvGU-V6",
        "outputId": "9efd1841-141a-4a34-cbdd-2015a6d0ad02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine_simples: 1.0000000000000002\n",
            "cosine_simples: 0.47140452079103173\n",
            "cosine_simples: 0.21081851067789195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/movies.csv\",\n",
        "        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/ratings.csv\",\n",
        "        \"https://raw.githubusercontent.com/GabrielFCosta/preprocessamento/main/tags.csv\"]\n",
        "datasets = []\n",
        "auxlist =[]\n",
        "for url in urls:\n",
        "  datasets.append(pd.read_csv(url))\n",
        "# concatena títulos e gêneros na coluna string\n",
        "datasets[0]['string'] = datasets[0]['title'] + \" \" + datasets[0]['genres']\n",
        "# substitui barras verticais '|' por espaços entre gêneros \n",
        "aux =''\n",
        "for idx1, i in datasets[0].iterrows():\n",
        "  aux = i['string'].replace('|', \" \")\n",
        "  auxlist.append(aux)\n",
        "datasets[0]['string'] = auxlist\n",
        "auxlist.clear()\n",
        "# concatena tags de usuários\n",
        "for idx1, i in datasets[0].iterrows():\n",
        "  df = datasets[2].loc[datasets[2]['movieId'] == i['movieId'] ] \n",
        "  aux =''\n",
        "  for idx2, j in df.iterrows():\n",
        "    aux = aux +\" \"+ j['tag']\n",
        "  #print(\"movie:\"+str(i['movieId'])+\" tags:\"+aux)\n",
        "  auxlist.append(aux)\n",
        "datasets[0]['string'] = datasets[0]['string'] + \"\" + auxlist\n",
        "\n"
      ],
      "metadata": {
        "id": "KPnYyw4MdM1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtra todos os filmes com rating máximo de determninado usuário\n",
        "id = random.randrange(1, 610)\n",
        "user = datasets[1].loc[(datasets[1]['userId'] == id) & (datasets[1]['rating'] == 5.0)]\n",
        "if user.shape[0] > 10:\n",
        "  # sorteia 11 filmes do usuário\n",
        "  usersample = user.sample(n=11)\n",
        "\n",
        "  # pega todos os ids de filmes\n",
        "  mids = datasets[0]['movieId']\n",
        "  # exclui os 11 filmes do usuário do total de filmes\n",
        "  mids = mids[mids.isin(usersample['movieId'].tolist()) == False]\n",
        "  # sorteia 40 outros filmes \n",
        "  mids = mids.sample(n=40)\n",
        "  mids = mids.tolist()\n",
        "\n",
        "  # desses 11, sorteia 1 para servir de referência e o exclui dos outros 11\n",
        "  reference = usersample.sample()\n",
        "  refmovie = reference['movieId'].iloc[0]\n",
        "  index = usersample[usersample['movieId'] == refmovie ].index\n",
        "  usersample = usersample.drop(index)\n",
        "  usersample = usersample['movieId'].tolist()\n",
        "\n",
        "  # junta duas listas de 10 filmes de rating 5 com outros 40 aleatórios\n",
        "  mids = mids + usersample\n",
        "  print(refmovie)\n",
        "  print(len(mids))\n",
        "  # pega filme de referência na base de filmes\n",
        "  aux = datasets[0][datasets[0]['movieId'] == refmovie]\n",
        "  #print(preprocess(aux['string'].iloc[0]))\n",
        "# usuário deve ter mais de 10 filmes com rating 5\n",
        "else:\n",
        "  print(\"Usuário não avaliou filmes suficientes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-_wgIhHMee5",
        "outputId": "e178ce4b-20d3-4571-8a4c-19a579f60437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6333\n",
            "50\n"
          ]
        }
      ]
    }
  ]
}